{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Readings and the Processed Readings to the Mongo Database\n",
    "\n",
    "Many libraries are required for this step. Most of these are required for converting the sensor readings into magnetometer values. The aberystwyth magnetometer samba server is also required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/mag_srv_env/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2082844800.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn import mixture\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import frg_single_fgm3_dat_insert as frg_insert\n",
    "\n",
    "from cpdetect import cpDetector\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import mongo_cursors\n",
    "\n",
    "import json\n",
    "import igrf12\n",
    "\n",
    "# Set the path of the mag_data samba share mount, or path to a copy of the samba share data.\n",
    "dat_fp = \"../mag_data/\"\n",
    "\n",
    "# Setup the time for converting NI time to UNIX time.\n",
    "NI_zerotime = np.datetime64('1904-01-01T00:00:00.000Z').astype('datetime64[s]').astype(np.float64)\n",
    "NI_zerotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the pre-2014 data, with corrected times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded:\n",
      "../mag_data/2013/3000000.CSV\n",
      "../mag_data/2013/2020000.CSV\n",
      "../mag_data/2013/2000000.CSV\n",
      "../mag_data/2013/1960000.CSV\n",
      "../mag_data/2013/0810000.CSV\n",
      "../mag_data/2013/3280000.CSV\n",
      "../mag_data/2013/0321202.CSV\n",
      "../mag_data/2013/0830000.CSV\n",
      "../mag_data/2013/0820000.CSV\n",
      "../mag_data/2013/0310000.CSV\n",
      "../mag_data/2013/1360000.CSV\n",
      "../mag_data/2013/2070000.CSV\n",
      "../mag_data/2013/0550000.CSV\n",
      "../mag_data/2013/0930000.CSV\n",
      "../mag_data/2013/2300000.CSV\n",
      "../mag_data/2013/2290000.CSV\n",
      "../mag_data/2013/3560000.CSV\n",
      "../mag_data/2013/1260000.CSV\n",
      "../mag_data/2013/2720000.CSV\n",
      "../mag_data/2013/0900000.CSV\n",
      "../mag_data/2013/2350000.CSV\n",
      "../mag_data/2013/3170000.CSV\n",
      "../mag_data/2013/2880000.CSV\n",
      "../mag_data/2013/1920000.CSV\n",
      "../mag_data/2013/1730000.CSV\n",
      "../mag_data/2013/0691221.CSV\n",
      "../mag_data/2013/1340000.CSV\n",
      "../mag_data/2013/1970000.CSV\n",
      "../mag_data/2013/3610000.CSV\n",
      "../mag_data/2013/2420000.CSV\n",
      "../mag_data/2013/1090000.CSV\n",
      "../mag_data/2013/2900000.CSV\n",
      "../mag_data/2013/2540000.CSV\n",
      "../mag_data/2013/3220000.CSV\n",
      "../mag_data/2013/1940000.CSV\n",
      "../mag_data/2013/2180000.CSV\n",
      "../mag_data/2013/2090000.CSV\n",
      "../mag_data/2013/3080000.CSV\n",
      "../mag_data/2013/1370000.CSV\n",
      "../mag_data/2013/2190000.CSV\n",
      "../mag_data/2013/0460914.CSV\n",
      "../mag_data/2013/3150000.CSV\n",
      "../mag_data/2013/2920000.CSV\n",
      "../mag_data/2013/0680000.CSV\n",
      "../mag_data/2013/1790000.CSV\n",
      "../mag_data/2013/0630000.CSV\n",
      "../mag_data/2013/3640000.CSV\n",
      "../mag_data/2013/3130000.CSV\n",
      "../mag_data/2013/1900000.CSV\n",
      "../mag_data/2013/0400000.CSV\n",
      "../mag_data/2013/2830000.CSV\n",
      "../mag_data/2013/3040000.CSV\n",
      "../mag_data/2013/1350000.CSV\n",
      "../mag_data/2013/1240000.CSV\n",
      "../mag_data/2013/0220000.CSV\n",
      "../mag_data/2013/0690000.CSV\n",
      "../mag_data/2013/3420000.CSV\n",
      "../mag_data/2013/3090000.CSV\n",
      "../mag_data/2013/1520000.CSV\n",
      "../mag_data/2013/1300000.CSV\n",
      "../mag_data/2013/2790000.CSV\n",
      "../mag_data/2013/0040000.CSV\n",
      "../mag_data/2013/0320000.CSV\n",
      "../mag_data/2013/3260000.CSV\n",
      "../mag_data/2013/0500000.CSV\n",
      "../mag_data/2013/0290000.CSV\n",
      "../mag_data/2013/1180000.CSV\n",
      "../mag_data/2013/0360000.CSV\n",
      "../mag_data/2013/1810000.CSV\n",
      "../mag_data/2013/2850000.CSV\n",
      "../mag_data/2013/3230000.CSV\n",
      "../mag_data/2013/1080000.CSV\n",
      "../mag_data/2013/3160000.CSV\n",
      "../mag_data/2013/0240000.CSV\n",
      "../mag_data/2013/0670000.CSV\n",
      "../mag_data/2013/3480000.CSV\n",
      "../mag_data/2013/3490000.CSV\n",
      "../mag_data/2013/0160000.CSV\n",
      "../mag_data/2013/2170000.CSV\n",
      "../mag_data/2013/1710000.CSV\n",
      "../mag_data/2013/2460000.CSV\n",
      "../mag_data/2013/0871115.CSV\n",
      "../mag_data/2013/1120000.CSV\n",
      "../mag_data/2013/2210000.CSV\n",
      "../mag_data/2013/3570000.CSV\n",
      "../mag_data/2013/1200000.CSV\n",
      "../mag_data/2013/3050000.CSV\n",
      "../mag_data/2013/1560000.CSV\n",
      "../mag_data/2013/0070000.CSV\n",
      "../mag_data/2013/1840000.CSV\n",
      "../mag_data/2013/3210000.CSV\n",
      "../mag_data/2013/1680000.CSV\n",
      "../mag_data/2013/3520000.CSV\n",
      "../mag_data/2013/0340000.CSV\n",
      "../mag_data/2013/0060000.CSV\n",
      "../mag_data/2013/2100000.CSV\n",
      "../mag_data/2013/2610000.CSV\n",
      "../mag_data/2013/3180000.CSV\n",
      "../mag_data/2013/2450000.CSV\n",
      "../mag_data/2013/1190000.CSV\n",
      "../mag_data/2013/1140000.CSV\n",
      "../mag_data/2013/2330000.CSV\n",
      "../mag_data/2013/0200000.CSV\n",
      "../mag_data/2013/2520000.CSV\n",
      "../mag_data/2013/1990000.CSV\n",
      "../mag_data/2013/0230000.CSV\n",
      "../mag_data/2013/3110000.CSV\n",
      "../mag_data/2013/0870000.CSV\n",
      "../mag_data/2013/0380000.CSV\n",
      "../mag_data/2013/2280000.CSV\n",
      "../mag_data/2013/1320000.CSV\n",
      "../mag_data/2013/0960000.CSV\n",
      "../mag_data/2013/1700000.CSV\n",
      "../mag_data/2013/2690000.CSV\n",
      "../mag_data/2013/1030000.CSV\n",
      "../mag_data/2013/0620000.CSV\n",
      "../mag_data/2013/0600000.CSV\n",
      "../mag_data/2013/1500000.CSV\n",
      "../mag_data/2013/0010000.CSV\n",
      "../mag_data/2013/1650000.CSV\n",
      "../mag_data/2013/2140000.CSV\n",
      "../mag_data/2013/2680000.CSV\n",
      "../mag_data/2013/3030000.CSV\n",
      "../mag_data/2013/3290000.CSV\n",
      "../mag_data/2013/2750000.CSV\n",
      "../mag_data/2013/1160000.CSV\n",
      "../mag_data/2013/1540000.CSV\n",
      "../mag_data/2013/3630000.CSV\n",
      "../mag_data/2013/0260000.CSV\n",
      "../mag_data/2013/0660000.CSV\n",
      "../mag_data/2013/2250000.CSV\n",
      "../mag_data/2013/0390000.CSV\n",
      "../mag_data/2013/2670000.CSV\n",
      "../mag_data/2013/0730000.CSV\n",
      "../mag_data/2013/0580000.CSV\n",
      "../mag_data/2013/0530000.CSV\n",
      "../mag_data/2013/2030000.CSV\n",
      "../mag_data/2013/3190000.CSV\n",
      "../mag_data/2013/2650000.CSV\n",
      "../mag_data/2013/1770000.CSV\n",
      "../mag_data/2013/0890000.CSV\n",
      "../mag_data/2013/1290000.CSV\n",
      "../mag_data/2013/3470000.CSV\n",
      "../mag_data/2013/3340000.CSV\n",
      "../mag_data/2013/2550000.CSV\n",
      "../mag_data/2013/2050000.CSV\n",
      "../mag_data/2013/0130000.CSV\n",
      "../mag_data/2013/0520000.CSV\n",
      "../mag_data/2013/0980000.CSV\n",
      "../mag_data/2013/0560000.CSV\n",
      "../mag_data/2013/3200000.CSV\n",
      "../mag_data/2013/1450000.CSV\n",
      "../mag_data/2013/0370000.CSV\n",
      "../mag_data/2013/1550000.CSV\n",
      "../mag_data/2013/2700000.CSV\n",
      "../mag_data/2013/1380000.CSV\n",
      "../mag_data/2013/2040000.CSV\n",
      "../mag_data/2013/1590000.CSV\n",
      "../mag_data/2013/1930000.CSV\n",
      "../mag_data/2013/2630000.CSV\n",
      "../mag_data/2013/2080000.CSV\n",
      "../mag_data/2013/0170000.CSV\n",
      "../mag_data/2013/3540000.CSV\n",
      "../mag_data/2013/0250000.CSV\n",
      "../mag_data/2013/2730000.CSV\n",
      "../mag_data/2013/1820000.CSV\n",
      "../mag_data/2013/1040000.CSV\n",
      "../mag_data/2013/3060000.CSV\n",
      "../mag_data/2013/1750000.CSV\n",
      "../mag_data/2013/2320000.CSV\n",
      "../mag_data/2013/3370000.CSV\n",
      "../mag_data/2013/0640000.CSV\n",
      "../mag_data/2013/0811016.CSV\n",
      "../mag_data/2013/1310000.CSV\n",
      "../mag_data/2013/0470000.CSV\n",
      "../mag_data/2013/1410000.CSV\n",
      "../mag_data/2013/1630000.CSV\n",
      "../mag_data/2013/2440000.CSV\n",
      "../mag_data/2013/0210000.CSV\n",
      "../mag_data/2013/0840000.CSV\n",
      "../mag_data/2013/1250000.CSV\n",
      "../mag_data/2013/3460000.CSV\n",
      "../mag_data/2013/2310000.CSV\n",
      "../mag_data/2013/0140000.CSV\n",
      "../mag_data/2013/3650000.CSV\n",
      "../mag_data/2013/1330000.CSV\n",
      "../mag_data/2013/0410000.CSV\n",
      "../mag_data/2013/2890000.CSV\n",
      "../mag_data/2013/1000000.CSV\n",
      "../mag_data/2013/0030000.CSV\n",
      "../mag_data/2013/3510000.CSV\n",
      "../mag_data/2013/1740000.CSV\n",
      "../mag_data/2013/0050000.CSV\n",
      "../mag_data/2013/1950000.CSV\n",
      "../mag_data/2013/3440000.CSV\n",
      "../mag_data/2013/2590000.CSV\n",
      "../mag_data/2013/0460000.CSV\n",
      "../mag_data/2013/0720000.CSV\n",
      "../mag_data/2013/2940000.CSV\n",
      "../mag_data/2013/2780000.CSV\n",
      "../mag_data/2013/3070000.CSV\n",
      "../mag_data/2013/2500000.CSV\n",
      "../mag_data/2013/2810000.CSV\n",
      "../mag_data/2013/2530000.CSV\n",
      "../mag_data/2013/1510000.CSV\n",
      "../mag_data/2013/0090000.CSV\n",
      "../mag_data/2013/1890000.CSV\n",
      "../mag_data/2013/0120000.CSV\n",
      "../mag_data/2013/1420000.CSV\n",
      "../mag_data/2013/1220000.CSV\n",
      "../mag_data/2013/0020000.CSV\n",
      "../mag_data/2013/1620000.CSV\n",
      "../mag_data/2013/1800000.CSV\n",
      "../mag_data/2013/3240000.CSV\n",
      "../mag_data/2013/0150000.CSV\n",
      "../mag_data/2013/3300000.CSV\n",
      "../mag_data/2013/3380000.CSV\n",
      "../mag_data/2013/0650000.CSV\n",
      "../mag_data/2013/3550000.CSV\n",
      "../mag_data/2013/2990000.CSV\n",
      "../mag_data/2013/3140000.CSV\n",
      "../mag_data/2013/0100000.CSV\n",
      "../mag_data/2013/1280000.CSV\n",
      "../mag_data/2013/2570000.CSV\n",
      "../mag_data/2013/1610000.CSV\n",
      "../mag_data/2013/3530000.CSV\n",
      "../mag_data/2013/3600000.CSV\n",
      "../mag_data/2013/1490000.CSV\n",
      "../mag_data/2013/2340000.CSV\n",
      "../mag_data/2013/0790000.CSV\n",
      "../mag_data/2013/3120000.CSV\n",
      "../mag_data/2013/0590000.CSV\n",
      "../mag_data/2013/0800000.CSV\n",
      "../mag_data/2013/3590000.CSV\n",
      "../mag_data/2013/0321007.CSV\n",
      "../mag_data/2013/0440000.CSV\n",
      "../mag_data/2013/2230000.CSV\n",
      "../mag_data/2013/2060000.CSV\n",
      "../mag_data/2013/1670000.CSV\n",
      "../mag_data/2013/0430000.CSV\n",
      "../mag_data/2013/2480000.CSV\n",
      "../mag_data/2013/0080000.CSV\n",
      "../mag_data/2013/1860000.CSV\n",
      "../mag_data/2013/1780000.CSV\n",
      "../mag_data/2013/1870000.CSV\n",
      "../mag_data/2013/0270000.CSV\n",
      "../mag_data/2013/3450000.CSV\n",
      "../mag_data/2013/3270000.CSV\n",
      "../mag_data/2013/1390000.CSV\n",
      "../mag_data/2013/2370000.CSV\n",
      "../mag_data/2013/1660000.CSV\n",
      "../mag_data/2013/3580000.CSV\n",
      "../mag_data/2013/2430000.CSV\n",
      "../mag_data/2013/1050000.CSV\n",
      "../mag_data/2013/1720000.CSV\n",
      "../mag_data/2013/1440000.CSV\n",
      "../mag_data/2013/0950000.CSV\n",
      "../mag_data/2013/0760000.CSV\n",
      "../mag_data/2013/1020000.CSV\n",
      "../mag_data/2013/2960000.CSV\n",
      "../mag_data/2013/1640000.CSV\n",
      "../mag_data/2013/0330000.CSV\n",
      "../mag_data/2013/0510000.CSV\n",
      "../mag_data/2013/1230000.CSV\n",
      "../mag_data/2013/2860000.CSV\n",
      "../mag_data/2013/0661429.CSV\n",
      "../mag_data/2013/3010000.CSV\n",
      "../mag_data/2013/3320000.CSV\n",
      "../mag_data/2013/0940000.CSV\n",
      "../mag_data/2013/0450000.CSV\n",
      "../mag_data/2013/3410000.CSV\n",
      "../mag_data/2013/2510000.CSV\n",
      "../mag_data/2013/3310000.CSV\n",
      "../mag_data/2013/1070000.CSV\n",
      "../mag_data/2013/2410000.CSV\n",
      "../mag_data/2013/1530000.CSV\n",
      "../mag_data/2013/0770000.CSV\n",
      "../mag_data/2013/2840000.CSV\n",
      "../mag_data/2013/3400000.CSV\n",
      "../mag_data/2013/2270000.CSV\n",
      "../mag_data/2013/1600000.CSV\n",
      "../mag_data/2013/2490000.CSV\n",
      "../mag_data/2013/1210000.CSV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../mag_data/2013/0490000.CSV\n",
      "../mag_data/2013/2110000.CSV\n",
      "../mag_data/2013/1110000.CSV\n",
      "../mag_data/2013/2010000.CSV\n",
      "../mag_data/2013/1470000.CSV\n",
      "../mag_data/2013/1010000.CSV\n",
      "../mag_data/2013/3360000.CSV\n",
      "../mag_data/2013/2220000.CSV\n",
      "../mag_data/2013/2760000.CSV\n",
      "../mag_data/2013/0280000.CSV\n",
      "../mag_data/2013/1130000.CSV\n",
      "../mag_data/2013/0990000.CSV\n",
      "../mag_data/2013/1980000.CSV\n",
      "../mag_data/2013/0350000.CSV\n",
      "../mag_data/2013/1100000.CSV\n",
      "../mag_data/2013/0351542.CSV\n",
      "Warning! Only 1 reading in the csv!\n",
      "../mag_data/2013/2710000.CSV\n",
      "../mag_data/2013/2260000.CSV\n",
      "../mag_data/2013/2970000.CSV\n",
      "../mag_data/2013/2580000.CSV\n",
      "../mag_data/2013/0190000.CSV\n",
      "../mag_data/2013/0710000.CSV\n",
      "../mag_data/2013/1760000.CSV\n",
      "../mag_data/2013/2910000.CSV\n",
      "../mag_data/2013/2930000.CSV\n",
      "../mag_data/2013/0850000.CSV\n",
      "../mag_data/2013/0910000.CSV\n",
      "../mag_data/2013/2240000.CSV\n",
      "../mag_data/2013/0570000.CSV\n",
      "../mag_data/2013/1460000.CSV\n",
      "../mag_data/2013/2560000.CSV\n",
      "../mag_data/2013/2200000.CSV\n",
      "../mag_data/2013/3350000.CSV\n",
      "../mag_data/2013/3500000.CSV\n",
      "../mag_data/2013/0740000.CSV\n",
      "../mag_data/2013/1150000.CSV\n",
      "../mag_data/2013/0420000.CSV\n",
      "../mag_data/2013/2470000.CSV\n",
      "../mag_data/2013/2380000.CSV\n",
      "../mag_data/2013/2740000.CSV\n",
      "../mag_data/2013/2360000.CSV\n",
      "../mag_data/2013/1880000.CSV\n",
      "../mag_data/2013/0970000.CSV\n",
      "../mag_data/2013/1690000.CSV\n",
      "../mag_data/2013/3250000.CSV\n",
      "../mag_data/2013/2120000.CSV\n",
      "../mag_data/2013/3390000.CSV\n",
      "../mag_data/2013/0860000.CSV\n",
      "../mag_data/2013/2870000.CSV\n",
      "../mag_data/2013/0811018.CSV\n",
      "../mag_data/2013/2620000.CSV\n",
      "../mag_data/2013/1400000.CSV\n",
      "../mag_data/2013/0610000.CSV\n",
      "../mag_data/2013/3620000.CSV\n",
      "../mag_data/2013/1910000.CSV\n",
      "../mag_data/2013/3330000.CSV\n",
      "../mag_data/2013/2660000.CSV\n",
      "../mag_data/2013/1580000.CSV\n",
      "../mag_data/2013/1850000.CSV\n",
      "../mag_data/2013/2800000.CSV\n",
      "../mag_data/2013/0750000.CSV\n",
      "../mag_data/2013/0180000.CSV\n",
      "../mag_data/2013/2820000.CSV\n",
      "../mag_data/2013/1170000.CSV\n",
      "../mag_data/2013/1060000.CSV\n",
      "../mag_data/2013/0480000.CSV\n",
      "../mag_data/2013/0700000.CSV\n",
      "../mag_data/2013/2980000.CSV\n",
      "../mag_data/2013/0110000.CSV\n",
      "../mag_data/2013/1430000.CSV\n",
      "../mag_data/2013/2150000.CSV\n",
      "../mag_data/2013/2400000.CSV\n",
      "../mag_data/2013/2130000.CSV\n",
      "../mag_data/2013/2640000.CSV\n",
      "../mag_data/2013/0780000.CSV\n",
      "../mag_data/2013/1830000.CSV\n",
      "../mag_data/2013/2600000.CSV\n",
      "../mag_data/2013/1480000.CSV\n",
      "../mag_data/2013/0300000.CSV\n",
      "../mag_data/2013/0920000.CSV\n",
      "../mag_data/2013/3430000.CSV\n",
      "../mag_data/2013/2770000.CSV\n",
      "../mag_data/2013/2160000.CSV\n",
      "../mag_data/2013/1570000.CSV\n",
      "../mag_data/2013/1270000.CSV\n",
      "../mag_data/2013/2390000.CSV\n",
      "../mag_data/2013/0540000.CSV\n",
      "../mag_data/2013/2950000.CSV\n",
      "../mag_data/2013/0811019.CSV\n",
      "../mag_data/2013/3020000.CSV\n",
      "../mag_data/2013/3100000.CSV\n",
      "../mag_data/2013/0880000.CSV\n",
      "(10551602, 3) (10551602,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:05</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:08</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:11</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:14</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:17</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     reading  temperature\n",
       "time                                     \n",
       "2011-10-27 23:00:05  31896.0       21.343\n",
       "2011-10-27 23:00:08  31896.0       21.342\n",
       "2011-10-27 23:00:11  31896.0       21.333\n",
       "2011-10-27 23:00:14  31896.0       21.346\n",
       "2011-10-27 23:00:17  31896.0       21.356"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_2013_dat():\n",
    "    \"\"\"Loads data from the 2013 folder, in the Aberystwyth magnetometer samba share.\"\"\"\n",
    "    hour_change_2011 = {\"start\": datetime(year=2011, month=3, day=27, hour=1, minute=0, second=0),\n",
    "                        \"end\": datetime(year=2011, month=10, day=30, hour=2, minute=0, second=0)}\n",
    "    hour_change_2012 = {\"start\": datetime(year=2012, month=3, day=25, hour=1, minute=0, second=0),\n",
    "                        \"end\": datetime(year=2012, month=10, day=28, hour=2, minute=0, second=0)}\n",
    "    hour_change_2013 = {\"start\": datetime(year=2013, month=3, day=31, hour=1, minute=0, second=0),\n",
    "                        \"end\": datetime(year=2013, month=10, day=27, hour=2, minute=0, second=0)}\n",
    "\n",
    "    dataframes = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(dat_fp+\"2013\"):\n",
    "        print(\"Files loaded:\")\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".csv\"):\n",
    "                try:\n",
    "                    file = os.path.join(root, file)\n",
    "                    # Retrieve file modification time.\n",
    "                    date = datetime.fromtimestamp(os.path.getmtime(file))\n",
    "                    day_dat = pd.read_csv(file, names=(\"time\", \"reading\", \"temperature\"))\n",
    "                    print(file)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                else:\n",
    "                    # First, set the time to an integer, then to a string.\n",
    "                    day_dat[\"time\"] = day_dat[\"time\"].astype('int').astype(\"str\", copy=False)\n",
    "                    # Make sure all strings are 6 characters long, filled with zeros\n",
    "                    day_dat[\"time\"] = day_dat[\"time\"].apply(lambda dt : dt.zfill(6))\n",
    "                    # Convert strings to datetimes, formatted as \"%H%M%S\".\n",
    "                    day_dat[\"time\"] = pd.to_datetime(day_dat[\"time\"], format=\"%H%M%S\")\n",
    "                    # Include the current year, day, month in the datetimes, as retrieved from the file modification time.\n",
    "                    day_dat[\"time\"] = day_dat[\"time\"].apply(lambda dt: dt.replace(year=date.year, month=date.month, day=date.day))\n",
    "                    # Due to an error, the last recording in the csv's is for the next day.\n",
    "                    # The error can be verified by checking whether the last time is\n",
    "                    # earlier than the previous.\n",
    "                    # We need a try-except because the csv may only contain 1 item.\n",
    "                    try:\n",
    "                        if day_dat.iloc[-1, 0] < day_dat.iloc[-2, 0]:\n",
    "                            # if the last time is earlier than the previous, then add a day to the time.\n",
    "                            day_dat.iloc[-1, 0] += timedelta(days=1)\n",
    "                    except IndexError:\n",
    "                        print(\"Warning! Only 1 reading in the csv!\")\n",
    "                        \n",
    "                    dataframes.append(day_dat)\n",
    "\n",
    "    sensor_dat = pd.concat(dataframes, ignore_index=True)\n",
    "    # Readings were taken in GMT and BST. Make them all GMT (UTC).\n",
    "    # Find BST times.\n",
    "    BST_mask = ((sensor_dat[\"time\"] >= hour_change_2011[\"start\"]) & (sensor_dat[\"time\"] < hour_change_2011[\"end\"]))\n",
    "    BST_mask |= ((sensor_dat[\"time\"] >= hour_change_2012[\"start\"]) & (sensor_dat[\"time\"] < hour_change_2012[\"end\"]))\n",
    "    BST_mask |= ((sensor_dat[\"time\"] >= hour_change_2013[\"start\"]) & (sensor_dat[\"time\"] < hour_change_2013[\"end\"]))\n",
    "    print(sensor_dat.shape, BST_mask.shape)\n",
    "    # Subtract an hour from the bst times\n",
    "    sensor_dat.loc[BST_mask, \"time\"] -= timedelta(hours=1)\n",
    "    sensor_dat.set_index(\"time\", inplace=True)\n",
    "    \n",
    "    return sensor_dat\n",
    "\n",
    "all_mag_dat = load_2013_dat()\n",
    "all_mag_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all magnetometer data from after 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "c22f240c-4652-47f3-a3e3-d7a4b6bbdc69"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load: ../mag_data/2014/data.zip\n",
      "('Multiple files found in compressed zip file %s', \"['DATA059.csv', 'DATA060.csv', 'DATA061.csv', 'DATA062.csv', 'DATA063.csv', 'DATA064.csv', 'DATA065.csv', 'DATA066.csv', 'DATA067.csv', 'DATA068.csv', 'DATA069.csv', 'DATA070.csv', 'DATA071.csv', 'DATA072.csv', 'DATA073.csv', 'DATA074.csv', 'DATA075.csv', 'DATA076.csv', 'DATA077.csv', 'DATA078.csv', 'DATA079.csv', 'DATA080.csv', 'DATA081.csv', 'DATA082.csv', 'DATA083.csv', 'DATA085.csv', 'DATA086.csv', 'DATA087.csv']\")\n",
      "Could not load: ../mag_data/2014/.directory\n",
      "could not convert string to float: 'ViewMode=1'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:05</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:08</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:11</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:14</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-27 23:00:17</th>\n",
       "      <td>31896.0</td>\n",
       "      <td>21.356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     reading  temperature\n",
       "time                                     \n",
       "2011-10-27 23:00:05  31896.0       21.343\n",
       "2011-10-27 23:00:08  31896.0       21.342\n",
       "2011-10-27 23:00:11  31896.0       21.333\n",
       "2011-10-27 23:00:14  31896.0       21.346\n",
       "2011-10-27 23:00:17  31896.0       21.356"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-06-30 09:28:56.994999886</th>\n",
       "      <td>47453.0</td>\n",
       "      <td>2.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30 09:29:00.049000263</th>\n",
       "      <td>47455.0</td>\n",
       "      <td>2.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30 09:29:03.094999790</th>\n",
       "      <td>47456.0</td>\n",
       "      <td>2.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30 09:29:06.150000095</th>\n",
       "      <td>47455.0</td>\n",
       "      <td>2.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30 09:29:09.200999737</th>\n",
       "      <td>47454.0</td>\n",
       "      <td>2.712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reading  temperature\n",
       "time                                               \n",
       "2018-06-30 09:28:56.994999886  47453.0        2.704\n",
       "2018-06-30 09:29:00.049000263  47455.0        2.700\n",
       "2018-06-30 09:29:03.094999790  47456.0        2.707\n",
       "2018-06-30 09:29:06.150000095  47455.0        2.703\n",
       "2018-06-30 09:29:09.200999737  47454.0        2.712"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_data(folder):\n",
    "    \"\"\"Loads post 2013 data from the Aberystwyth magnetometer server.\"\"\"\n",
    "    #year = datetime(int(folder),1,1)\n",
    "    dataframes = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            try:\n",
    "                day_dat = pd.read_csv(os.path.join(root, file),\n",
    "                                      names=(\"time\", \"reading\", \"temperature\"),\n",
    "                                    dtype={\"time\": np.float64, \"reading\": np.float64, \"temperature\": np.float64})\n",
    "            except Exception as e:\n",
    "                print(\"Could not load:\", os.path.join(root, file))\n",
    "                print(e)\n",
    "            else:\n",
    "                # Convert NI time to UNIX time.\n",
    "                day_dat[\"time\"] = pd.to_datetime(day_dat[\"time\"] + NI_zerotime, unit='s')\n",
    "                dataframes.append(day_dat)\n",
    "\n",
    "    sensor_dat = pd.concat(dataframes, ignore_index=True)\n",
    "    sensor_dat.sort_values(\"time\", inplace=True)\n",
    "    sensor_dat.set_index(\"time\", inplace=True)\n",
    "\n",
    "    return sensor_dat\n",
    "\n",
    "\n",
    "for year_data in [\"2014\", \"2015\", \"2016\", \"2017\", \"2018\"]:\n",
    "    all_mag_dat = all_mag_dat.append(load_data(dat_fp+year_data))\n",
    "    \n",
    "all_mag_dat = all_mag_dat.loc[pd.notnull(all_mag_dat.index)]\n",
    "display(all_mag_dat.head())\n",
    "display(all_mag_dat.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed raw data to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index, so that timestamps are included in the upload.\n",
    "frg_insert.write_raw_dat(all_mag_dat.reset_index().to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process raw data into magnetometer data, and add IGRF data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:34:49 INFO cpDetector: =======================================\n",
      "16:34:49 INFO cpDetector: Running change point detector\n",
      "16:34:49 INFO cpDetector: =======================================\n",
      "16:34:49 INFO cpDetector:    input observations: 1 of length [25946]\n",
      "16:34:49 INFO cpDetector: Running cp detector on traj 0\n",
      "16:34:49 INFO cpDetector: ---------------------------------\n",
      "16:35:36 INFO cpDetector:     Found a new change point at: 8741!!\n",
      "16:35:46 INFO cpDetector:     Found a new change point at: 7264!!\n",
      "16:35:55 INFO cpDetector:     Found a new change point at: 2846!!\n",
      "16:35:58 INFO cpDetector:     Found a new change point at: 325!!\n",
      "16:36:01 INFO cpDetector:     Found a new change point at: 2512!!\n",
      "16:36:03 INFO cpDetector:     Found a new change point at: 965!!\n",
      "16:36:05 INFO cpDetector:     Found a new change point at: 1458!!\n",
      "16:36:06 INFO cpDetector:     Found a new change point at: 1203!!\n",
      "16:36:06 INFO cpDetector:     Found a new change point at: 1398!!\n",
      "16:36:08 INFO cpDetector:     Found a new change point at: 2297!!\n",
      "16:36:09 INFO cpDetector:     Found a new change point at: 2407!!\n",
      "16:36:14 INFO cpDetector:     Found a new change point at: 4900!!\n",
      "16:36:16 INFO cpDetector:     Found a new change point at: 4731!!\n",
      "16:36:18 INFO cpDetector:     Found a new change point at: 3002!!\n",
      "16:36:20 INFO cpDetector:     Found a new change point at: 3454!!\n",
      "16:36:22 INFO cpDetector:     Found a new change point at: 4419!!\n",
      "16:36:23 INFO cpDetector:     Found a new change point at: 4192!!\n",
      "16:36:23 INFO cpDetector:     Found a new change point at: 3946!!\n",
      "16:36:24 INFO cpDetector:     Found a new change point at: 3638!!\n",
      "16:36:28 INFO cpDetector:     Found a new change point at: 5994!!\n",
      "16:36:29 INFO cpDetector:     Found a new change point at: 5700!!\n",
      "16:36:30 INFO cpDetector:     Found a new change point at: 5880!!\n",
      "16:36:32 INFO cpDetector:     Found a new change point at: 6686!!\n",
      "16:36:32 INFO cpDetector:     Found a new change point at: 6610!!\n",
      "16:36:35 INFO cpDetector:     Found a new change point at: 7988!!\n",
      "16:36:36 INFO cpDetector:     Found a new change point at: 7869!!\n",
      "16:36:37 INFO cpDetector:     Found a new change point at: 8131!!\n",
      "16:37:04 INFO cpDetector:     Found a new change point at: 23070!!\n",
      "16:37:24 INFO cpDetector:     Found a new change point at: 10361!!\n",
      "16:37:26 INFO cpDetector:     Found a new change point at: 8883!!\n",
      "16:37:26 INFO cpDetector:     Found a new change point at: 8775!!\n",
      "16:37:28 INFO cpDetector:     Found a new change point at: 9643!!\n",
      "16:37:47 INFO cpDetector:     Found a new change point at: 17359!!\n",
      "16:37:55 INFO cpDetector:     Found a new change point at: 14427!!\n",
      "16:37:59 INFO cpDetector:     Found a new change point at: 11437!!\n",
      "16:38:04 INFO cpDetector:     Found a new change point at: 13493!!\n",
      "16:38:06 INFO cpDetector:     Found a new change point at: 12156!!\n",
      "16:38:12 INFO cpDetector:     Found a new change point at: 16323!!\n",
      "16:38:14 INFO cpDetector:     Found a new change point at: 15508!!\n",
      "16:38:24 INFO cpDetector:     Found a new change point at: 22903!!\n",
      "16:38:31 INFO cpDetector:     Found a new change point at: 21399!!\n",
      "16:38:35 INFO cpDetector:     Found a new change point at: 19697!!\n",
      "16:38:38 INFO cpDetector:     Found a new change point at: 18906!!\n",
      "16:38:40 INFO cpDetector:     Found a new change point at: 17992!!\n",
      "16:38:40 INFO cpDetector:     Found a new change point at: 17381!!\n",
      "16:38:44 INFO cpDetector:     Found a new change point at: 20772!!\n",
      "16:38:46 INFO cpDetector:     Found a new change point at: 21031!!\n",
      "16:38:48 INFO cpDetector:     Found a new change point at: 21928!!\n",
      "16:38:50 INFO cpDetector:     Found a new change point at: 22926!!\n",
      "16:38:50 INFO cpDetector:     Found a new change point at: 23002!!\n",
      "16:38:50 INFO cpDetector:     Found a new change point at: 22932!!\n",
      "16:38:53 INFO cpDetector:     Found a new change point at: 24044!!\n",
      "16:38:54 INFO cpDetector:     Found a new change point at: 23232!!\n",
      "16:38:57 INFO cpDetector:     Found a new change point at: 24914!!\n",
      "16:38:59 INFO cpDetector: Generating step fucntion\n",
      "16:38:59 INFO cpDetector: ---------------------------------\n",
      "16:38:59 INFO cpDetector: Elapsed time: 249.69955897331238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/mag_srv_env/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/james/anaconda3/envs/mag_srv_env/lib/python3.6/site-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/james/anaconda3/envs/mag_srv_env/lib/python3.6/site-packages/ipykernel_launcher.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/james/anaconda3/envs/mag_srv_env/lib/python3.6/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/james/anaconda3/envs/mag_srv_env/lib/python3.6/site-packages/ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.15301161e-10 -6.26616131e-09  5.68023145e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/mag_srv_env/lib/python3.6/site-packages/ipykernel_launcher.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/james/anaconda3/envs/mag_srv_env/lib/python3.6/site-packages/ipykernel_launcher.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.61263313e-10 -1.35302134e-08  5.92024355e-06]\n",
      "[ 3.43064448e-11 -4.13700804e-09  5.79381265e-06]\n",
      "[ 1.59300435e-10 -9.63982651e-09  5.87928158e-06]\n",
      "[ 7.28076462e-10 -3.48194196e-08  6.16373611e-06]\n",
      "[-2.04370299e-11 -2.47037555e-09  5.79837466e-06]\n",
      "[-3.51865477e-09  1.53862277e-07  4.04767591e-06]\n",
      "[ 1.52726392e-09 -6.56218887e-08  6.43697875e-06]\n",
      "[ 3.73042661e-10 -1.71566004e-08  5.81035624e-06]\n",
      "[-3.38098543e-10  1.23572393e-08  5.55137921e-06]\n",
      "[-7.42107127e-11 -3.24956176e-10  5.70515271e-06]\n",
      "[ 1.66923322e-09 -7.66297080e-08  6.54167462e-06]\n",
      "[ 6.95139675e-10 -3.46205489e-08  6.08911164e-06]\n",
      "[ 3.78727253e-10 -1.93868265e-08  5.90699756e-06]\n",
      "[ 7.54337390e-10 -3.79818267e-08  6.13558398e-06]\n",
      "[ 1.11965255e-09 -5.16072214e-08  6.25772490e-06]\n",
      "[-2.57227561e-09  1.09506226e-07  4.50449837e-06]\n",
      "[ 9.34142911e-11 -6.96077992e-09  5.77777067e-06]\n",
      "[-7.70888348e-11  1.15232612e-09  5.68110923e-06]\n",
      "[-7.90483551e-11 -5.57717780e-10  5.71863823e-06]\n",
      "[-5.27282555e-11 -2.58878581e-09  5.76163041e-06]\n",
      "[-2.67732077e-10  4.92007983e-09  5.69227320e-06]\n",
      "[-2.02741356e-10  4.12812726e-09  5.68226540e-06]\n",
      "[ 1.99772254e-10 -1.26694199e-08  5.90577487e-06]\n",
      "[ 7.21846098e-12 -6.28098576e-09  5.85252862e-06]\n",
      "[-9.27397229e-11 -4.00031910e-10  5.77843055e-06]\n",
      "[ 1.45800684e-10 -1.08480115e-08  5.89256453e-06]\n",
      "Expected n_samples >= n_components but got n_components = 2, n_samples = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/mag_srv_env/lib/python3.6/site-packages/scipy/optimize/minpack.py:794: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.45248656e-08 -2.00741437e-06  2.88519509e-05]\n",
      "[ 1.69479943e-08 -1.19255515e-06  2.58161254e-05]\n",
      "[ 1.99775854e-10 -1.28013764e-08  6.41658622e-06]\n",
      "[-1.20101425e-09  6.14215871e-08  5.62547975e-06]\n",
      "Improper input: N=3 must not exceed M=2\n",
      "[-2.08031087e-05  1.20661804e-03 -1.74897328e-02]\n",
      "[-2.32604834e-10  5.56416346e-09  6.22745069e-06]\n",
      "[ 1.40608202e-10 -1.32761104e-08  6.76261177e-06]\n",
      "[ 2.94071988e-09 -9.98488429e-08  7.42562882e-06]\n",
      "[ 5.08371134e-10 -2.75688405e-08  6.90099097e-06]\n",
      "[ 1.49838035e-10 -1.46166962e-08  6.79322430e-06]\n",
      "[ 7.95757631e-11 -8.95805896e-09  5.91594767e-06]\n",
      "[ 1.43868149e-10 -1.21179617e-08  5.96571309e-06]\n",
      "[ 8.04749833e-11 -9.11393634e-09  5.92812109e-06]\n",
      "[ 1.50961272e-10 -1.16266142e-08  5.94561910e-06]\n",
      "[ 1.06810546e-10 -9.85111323e-09  5.91872222e-06]\n",
      "[-8.01727326e-10  2.17813492e-08  5.64383555e-06]\n",
      "[ 1.23730128e-10 -1.03891794e-08  5.91865033e-06]\n",
      "[ 6.90018478e-11 -8.05617937e-09  5.90181559e-06]\n",
      "[-6.95307128e-13 -4.38878834e-09  5.85723824e-06]\n",
      "[-1.64703531e-11 -2.79899511e-09  5.81945351e-06]\n",
      "[ 1.50033244e-08 -7.44410286e-07  1.48178199e-05]\n",
      "[ 1.82246169e-11 -3.83209377e-09  5.62563421e-06]\n",
      "[ 2.00822701e-10 -1.30573102e-08  5.74750680e-06]\n",
      "[ 2.18684926e-10 -1.15476707e-08  5.67594783e-06]\n",
      "[ 3.47701777e-11 -6.21678261e-09  5.63866881e-06]\n",
      "[ 2.20530227e-10 -1.17890171e-08  5.68266246e-06]\n",
      "[ 1.58443996e-10 -9.92603032e-09  5.66579831e-06]\n",
      "[ 4.94989253e-11 -5.26843309e-09  5.61392961e-06]\n",
      "Improper input: N=3 must not exceed M=2\n",
      "[ 1.21246335e-09 -5.09655085e-08  6.06270088e-06]\n",
      "[-6.11454295e-10  1.09662356e-08  5.53846461e-06]\n",
      "[ 1.64153035e-10 -1.12396528e-08  5.69193935e-06]\n",
      "[ 2.14826385e-10 -1.26879594e-08  5.69346406e-06]\n",
      "[ 2.17982219e-10 -1.31551091e-08  5.69788610e-06]\n",
      "[ 4.96133129e-11 -5.77173769e-09  5.61804588e-06]\n",
      "[ 2.29456219e-10 -1.39402249e-08  5.70811454e-06]\n",
      "[ 6.98903543e-11 -6.74280234e-09  5.62483738e-06]\n",
      "[ 3.05926803e-11 -4.68356378e-09  5.59542684e-06]\n",
      "[ 1.83023731e-10 -1.27016032e-08  5.69959472e-06]\n",
      "[ 2.18316375e-10 -1.42698363e-08  5.71169030e-06]\n",
      "Expected n_samples >= n_components but got n_components = 2, n_samples = 1\n",
      "[-8.13440661e-09  4.93353960e-07 -2.53897976e-06]\n",
      "[ 3.08037419e-10 -2.79735007e-08  5.69830244e-06]\n",
      "[-3.11004381e-11  2.48229130e-09  5.11223073e-06]\n",
      "[-2.71057375e-11  2.68312763e-09  5.08832270e-06]\n",
      "[-1.00774460e-10 -4.01164676e-09  6.49732124e-06]\n",
      "[ 4.57005389e-12 -7.69869018e-09  6.52160048e-06]\n",
      "[-3.06815877e-11 -6.56001488e-09  6.51434818e-06]\n",
      "[-8.00340053e-11 -3.29720954e-09  6.46882774e-06]\n",
      "[-1.74891587e-11 -7.08323141e-09  6.52735968e-06]\n",
      "[ 1.57313296e-10 -1.55488846e-08  6.63107532e-06]\n"
     ]
    }
   ],
   "source": [
    "mag_dat_1h = all_mag_dat.resample('1h').mean()\n",
    "mag_dat_1h.dropna(inplace=True)\n",
    "all_mag_dat = all_mag_dat.resample('1T').mean()\n",
    "all_mag_dat.dropna(inplace=True)\n",
    "\n",
    "## Splitting the data into \n",
    "detector = cpDetector([mag_dat_1h[\"reading\"]], distribution='log_normal', log_odds_threshold=0)\n",
    "detector.detect_cp()\n",
    "# Get the changepoint times.\n",
    "cp_times = mag_dat_1h.index[detector.change_points[\"traj_0\"].ts]\n",
    "# set the minimum difference for a time discontinuity to be considered \"significant\". \n",
    "sig_diff = np.timedelta64(1, 'D')\n",
    "# Select the times where there are significant discontinuities.\n",
    "discs = all_mag_dat.dropna().index[np.append([False], np.diff(all_mag_dat.dropna().index) > sig_diff)]\n",
    "# Combine the changepoint and discontinuity times using a union.\n",
    "split_times = np.union1d(cp_times, discs)\n",
    "# Sort the times.\n",
    "split_times.sort()\n",
    "# Append the very first time from all the magnetometer data and the very last time.\n",
    "# This is so when performing calculations, the magnetometer data between the very\n",
    "# start/ end and the first/ last changepoint/ discontinuity is included.\n",
    "split_times = np.append([np.datetime64(all_mag_dat.index[0])], split_times)\n",
    "split_times = np.append(split_times, [np.datetime64(all_mag_dat.index[-1])])\n",
    "\n",
    "## Setting up the data for converting into magnetometer data.\n",
    "# Set the bias.\n",
    "bias = 90000\n",
    "# Calculate the period.\n",
    "all_mag_dat[\"period\"] = np.power(((-all_mag_dat[\"reading\"]*70/65534+120)*1000+bias), -1)\n",
    "# Create a place to store the \"corrected\" temperature.\n",
    "all_mag_dat[\"temp_corr\"] = all_mag_dat.loc[:, \"temperature\"]\n",
    "# create places to store the period with the temperature correction and the magnetic field.\n",
    "all_mag_dat[\"period_temp_corr\"] = np.nan\n",
    "all_mag_dat[\"F\"] = np.nan\n",
    "\n",
    "# Set Timur Zagidulin's function which converts the \"period\" readings into magnetometer readings (in nT).\n",
    "coefs = np.vstack(([ 7.75627250e+06, -5.60893614e+06, 1.61438337e+06, -2.30773456e+05,\n",
    "                    1.64006851e+04, -4.67419417e+02],\n",
    "                     [ 1.37845775e+07, -1.08586181e+07, 3.43274556e+06, -5.43192224e+05,\n",
    "                      4.30466903e+04, -1.37019412e+03],\n",
    "                     [ 9.93009467e+06, -7.05978073e+06, 1.98324201e+06, -2.74458051e+05,\n",
    "                      1.87117495e+04, -5.06023333e+02])).mean(axis=0)\n",
    "\n",
    "def mag_field(period_dat, coefs=coefs):\n",
    "    return np.polynomial.polynomial.polyval(period_dat*1e6, coefs)\n",
    "\n",
    "## Perform corrections and conversions.\n",
    "def quad_fun(x, a, b, c):\n",
    "    return a*x*x + b*x + c\n",
    "\n",
    "\n",
    "def corr_temp(mag_dat):\n",
    "    X = np.column_stack((mag_dat[\"temperature\"], mag_dat[\"period\"]))\n",
    "    gmm = mixture.GaussianMixture(n_components=2,\n",
    "                                  covariance_type='full',\n",
    "                                  means_init=((15, 0.000012), (0, 0.00001)),\n",
    "                                 random_state=100).fit(X)\n",
    "\n",
    "    temp_loopbacks = gmm.predict(X).astype(np.bool)\n",
    "    mag_dat.loc[temp_loopbacks, \"temp_corr\"] += mag_dat[\"temperature\"].max()\n",
    "    return mag_dat[\"temp_corr\"], temp_loopbacks\n",
    "\n",
    "\n",
    "def corr_period_temp(mag_dat, fun):\n",
    "#     popt, pcov = curve_fit(fun, mag_dat[\"temp_corr\"],\n",
    "#                      mag_dat[\"period\"],\n",
    "#                      p0=(1e-7, -1e-2, 1e-6),\n",
    "#                      bounds=((0,-1e6,-1e6), (1e6, 0, 1e6)),\n",
    "#                      maxfev=1000)\n",
    "    popt, pcov = curve_fit(fun, mag_dat[\"temp_corr\"], mag_dat[\"period\"], maxfev=1000)\n",
    "    temp_period_fit = fun(mag_dat[\"temp_corr\"], *popt)\n",
    "    mag_dat[\"period_temp_corr\"] = mag_dat[\"period\"] - temp_period_fit + mag_dat[\"period\"].mean()\n",
    "    return popt, pcov, mag_dat[\"period_temp_corr\"]\n",
    "\n",
    "\n",
    "def plot_results(mag_dat, popt, temp_loopbacks, fun):\n",
    "    temp_period_fit = fun(mag_dat[\"temp_corr\"], *popt)\n",
    "    fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    ax[0].set_title(\"Temperature vs Period between\\n\" + str(mag_dat.index.min()) + \" and \" + str(mag_dat.index.max()))\n",
    "    ax[0].set_ylim((mag_dat[\"period\"].min(), mag_dat[\"period\"].max()))\n",
    "    ax[0].scatter(mag_dat[\"temperature\"], mag_dat[\"period\"], c=temp_loopbacks, s=1)\n",
    "    \n",
    "    ax[1].set_title(\"Corrected Temperature vs Period between\\n\" + str(mag_dat.index.min()) + \" and \" + str(mag_dat.index.max()))\n",
    "    ax[1].set_ylim((mag_dat[\"period\"].min(), mag_dat[\"period\"].max()))\n",
    "    ax[1].scatter(mag_dat[\"temp_corr\"],mag_dat[\"period\"], c=temp_loopbacks, s=1)\n",
    "    mag_dat[\"temp_fit\"] = fun(mag_dat[\"temp_corr\"], *popt)\n",
    "    mag_dat.sort_values(\"temp_corr\", inplace=True)\n",
    "    ax[1].plot(mag_dat[\"temp_corr\"], mag_dat[\"temp_fit\"])\n",
    "    plt.show()\n",
    "    mag_dat.drop(\"temp_fit\", axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "def get_mag_field(mag_dat, temp_fun=quad_fun, plots=True): \n",
    "    mag_dat[\"temp_corr\"], temp_loopbacks = corr_temp(mag_dat)\n",
    "    \n",
    "    popt, pcov, mag_dat[\"period_temp_corr\"] = corr_period_temp(mag_dat, temp_fun)\n",
    "    print(popt)\n",
    "    mag_dat[\"F\"] = mag_field(mag_dat[\"period_temp_corr\"])\n",
    "    mag_dat[\"F\"] -= mag_dat[\"F\"].mean()\n",
    "    \n",
    "    if plots:\n",
    "        plot_results(mag_dat, popt, temp_loopbacks, temp_fun)\n",
    "    \n",
    "    return mag_dat\n",
    "\n",
    "for i in range(split_times.shape[0] - 1):\n",
    "    try:\n",
    "        all_mag_dat.loc[split_times[i]:split_times[i+1]] = get_mag_field(all_mag_dat.loc[split_times[i]:split_times[i+1]], plots=False)   \n",
    "    except (ValueError, TypeError, RuntimeError) as e:\n",
    "        print(e)\n",
    "    \n",
    "## Get IGRF values.\n",
    "# Open the frongoch sensor details.\n",
    "with open(\"frongoch_sensor_details.json\", 'r') as frg_inf_file:\n",
    "    frg_info = json.load(frg_inf_file)\n",
    "\n",
    "# Downsample, because igrf results don't significantly over 1 day or less and the model is slow to process.\n",
    "mag_dat_1d_times = all_mag_dat.resample(\"1d\").first()\n",
    "# Get results from model. This only selects the magnitude of the magnetic field from the data.\n",
    "mag_dat_1d_times[\"igrf\"] = mag_dat_1d_times.index.map(lambda dt: \n",
    "                                                      float(igrf12.igrf(dt,\n",
    "                                                                      glat=frg_info[\"latitude\"],\n",
    "                                                                      glon=frg_info[\"longitude\"],\n",
    "                                                                      alt_km=frg_info[\"elevation\"]/1000)[\"total\"])).astype(np.float32)\n",
    "# Upsample, and add to original data\n",
    "all_mag_dat[\"igrf\"] = mag_dat_1d_times[\"igrf\"].resample('1T').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert processed Frongoch magnetometer data to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frg_insert.write_mag_dat(all_mag_dat[[\"F\", \"igrf\"]].reset_index().to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded, the [server](../server), for retrieving magnetometer data, needs setting up."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
